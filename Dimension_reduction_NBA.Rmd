---
title: "Dimension reduction with PCA"
author: "Adam Dudek"
date: "23.02.2021"
output:
  prettydoc::html_pretty:
    theme: architect
    highlight: github
---

## Dimension reduction with PCA


### What is dimension reduction and why it is useful?

#### Dimension reduction is a type of process of data transformation, where number of variables in the model is reduceded and a new set of new variables is created in the model. In other words, it is a transformation of data from high-dimensional space into a low-dimensional space without loss of a lot meaningfull informations from original dataset.
It helps analysing data much faster, as working in high-dimensional datasets is often computationally intractable and more time consuming.
Nowadays dimensionality reduction is common in fields that deal with large numbers of observations and can be used for noise reduction, data visualization, cluster analysis, or as an intermediate step to facilitate other analyses.

#### Methods of dimnension reduction are commonly divided into linear and non-linear approaches. Approaches can also be divided into:

- feature selection approach, where we try to find a subset of the input variables

- feature extraction approach, where we try to reduce number of dimensions, by  building a new feature set from the original feature set either by generating features which are composites of existing features or by some other logic.

There are two common methods to perform feature extraction. They are,
Principal component analysis (PCA) & Linear discriminant analysis (LDA).

#### In this paper Im gonna explain how one of the most popular linear data transformation, Principal Component Analysis, works and present its outcome while analysing the photo.



## Principal Component analysis
#### As it was mentioned before, PCA is a dimensionality-reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set.

Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process. *

### HOW PCA CONSTRUCTS THE PRINCIPAL COMPONENTS?
As there are as many principal components as there are variables in the data, principal components are constructed in such a manner that the first principal component accounts for the largest possible variance in the data set. For example, lets assume that the scatter plot of our data set is as shown below, can we guess the first principal component ? Yes, its approximately the line that matches the purple marks because it goes through the origin and its the line in which the projection of the points (red dots) is the most spread out. Or mathematically speaking, its the line that maximizes the variance (the average of the squared distances from the projected points (red dots) to the origin).

The second principal component is calculated in the same way, with the condition that it is uncorrelated with (i.e., perpendicular to) the first principal component and that it accounts for the next highest variance.

![Alt Text](https://media.giphy.com/media/iWlCmOJ0mIewdoND5J/giphy.gif)




### PCA for NBA data
Lets say we want to check how PCA will perform on data from NBA games. We have a final table of 2019 NBA results. 
```{r}
X <- read.csv("https://drive.google.com/uc?export=download&id=1iv6Rf5rbK4RM3gNl1_ADsfp2EEn0LIfm", sep=";",dec=",",header=T, stringsAsFactors=F)
X<-X[2:30,1:28]
head(X)
```

We want to analyse, how precice basics statistics, like rebounds, fouls, turnovers ect., can help us predict an outcome of the game. To do it linear regression model is conducted.
Our independet variable will be % of wins, that given team had during the season. Rest of the variables will be dependent variables. Of couse we will not take into account variables like wins and loses, as these two variables will explain 100% of varianiace.
```{r, echo=FALSE}
#ten ma byc niewidoczny
X$PFD<- as.numeric(as.numeric(gsub(",", ".",X$PFD)))
X$PF<- as.numeric(as.numeric(gsub(",", ".",X$PF)))
X$BLKA<- as.numeric(as.numeric(gsub(",", ".",X$BLKA)))
X$BLK<- as.numeric(as.numeric(gsub(",", ".",X$BLK)))
X$STL<- as.numeric(as.numeric(gsub(",", ".",X$STL)))
X$TOV<- as.numeric(as.numeric(gsub(",", ".",X$TOV)))
X$AST<- as.numeric(as.numeric(gsub(",", ".",X$AST)))
sum(is.na(X))
```

```{r}
X <- X[,c("WIN.", "MIN","PTS",  "FGM",  "FGA" , "FG." , "X3PM", "X3PA","X3P.", "FTM","FTA" , "FT.", "OREB", "DREB", "REB"  ,"AST", "TOV",  "STL",  "BLK" , "BLKA" ,"PF"  , "PFD")]

lin_model<- lm(WIN.~.,data=X)
summary(lin_model)

```
As we can see basic statistics explain 90.6% of variance. It is a good result, while luck in sport plays a huge role.
To obtain this result 21 variables were used. 
Now e will try to perform PCA analysis.


```{r}

PCA_NBA<- prcomp(X,center = TRUE,scale=TRUE)
summary(PCA_NBA)

```

Recall that a property of PCA is that our components are sorted from largest to smallest with regard to their standard deviation (Eigenvalues). So lets make sense of these:

- Standard deviation: This is simply the eigenvalues in our case since the data has been centered and scaled (standardized)
- Proportion of Variance: This is the amount of variance the component accounts for in the data, ie. PC1 accounts for 25% of total variance in the data alone!
- Cumulative Proportion: This is simply the accumulated amount of explained variance, ie. if we used the first 10 components we would be able to account for >93% of total variance in the data.

```{r}
#niewidoczne
Cumulative_variance <- data.frame(c("PC1","PC2","PC3","PC4","PC5","PC6","PC7","PC8","PC9"), c(0.2495, 0.4184, 0.5432 ,0.64042 ,0.71125 ,0.77776, 0.82777 ,0.8721, 0.90346))
colnames(Cumulative_variance)<- c("PC","Cumulative_Variance_explained" )

```

```{r}
library(ggplot2)
ggplot(Cumulative_variance, aes(x=PC, y=Cumulative_Variance_explained)) + 
geom_bar(stat= "identity",color="black",fill = heat.colors(9), width=0.7)+
ggtitle("Cumulative Variance Explained")
```

First 5 components from PCA is responsible for 71,5% of variance and by using first 10 components or model explain bigger percenage of variance (94%), than linear regression model.

When we see how PCA can help us reduce multidimensionality of data, lets have a look how it can be used to compress image.




## Image Compression with Principal Component Analysis

I will try to reduce an old photo of Michael Jordan using PCA. The photo size is 641x700 pixels.
```{r,include=FALSE,echo=FALSE}

library(imager)
library(here)
library(dplyr)
library(broom)
library(ggplot2)
library(tidyverse)
library(jpeg)

```

```{r, echo=TRUE, results='hide' }
myurl <- "https://drive.google.com/uc?export=download&id=1CNZedLaQMLXED_2qPBP-ZRex1bMPuP7g"
z <- tempfile()
download.file(myurl,z,mode="wb")
MJ <- readJPEG(z)
file.remove(z)
MJ_df<-as.data.frame(t(MJ))
str(MJ)
dim(MJ)
```
After inserting data, I transformed it to data frame and created a matrix with a size of image using function pivot_wider.

Next step was to conduct a PCA.

```{r}
MJ_pca <-prcomp(MJ_df[,-1],scale = TRUE, center = TRUE)
```

```{r,include=FALSE,echo=FALSE}
reverse_pca <- function(n_comp = 20, pca_object = MJ_pca){
  ## The pca_object is an object created by base R's prcomp() function.
  
  ## Multiply the matrix of rotated data by the transpose of the matrix 
  ## of eigenvalues (i.e. the component loadings) to get back to a 
  ## matrix of original data values
  recon <- pca_object$x[, 1:n_comp] %*% t(pca_object$rotation[, 1:n_comp])
  
  ## Reverse any scaling and centering that was done by prcomp()
  
  if(all(pca_object$scale != FALSE)){
    ## Rescale by the reciprocal of the scaling factor, i.e. back to
    ## original range.
    recon <- scale(recon, center = FALSE, scale = 1/pca_object$scale)
  }
  if(all(pca_object$center != FALSE)){
    ## Remove any mean centering by adding the subtracted mean back in
    recon <- scale(recon, scale = FALSE, center = -1 * pca_object$center)
  }
  
  ## Make it a data frame that we can easily pivot to long format
  ## (because that's the format that the excellent imager library wants
  ## when drawing image plots with ggplot)
  recon_df <- data.frame(cbind(1:nrow(recon), recon))
  colnames(recon_df) <- c("x", 1:(ncol(recon_df)-1))

  ## Return the data to long form 
  recon_df_long <- recon_df %>%
    tidyr::pivot_longer(cols = -x, 
                        names_to = "y", 
                        values_to = "value") %>%
    mutate(y = as.numeric(y)) %>%
    arrange(y) %>%
    as.data.frame()
  
  recon_df_long
}
```


To obtain photo we need to reverse obtained results into matrix with data that will let us generate a photo.
How exactly this mechanism works is explained in the attached link**, which you will find below. In general, when we perform PCA by prcomp() function we obtain a list with 5 lists inside:"sdev","rotation", "center", "scale" and "x" .
To recieve back informations to our matrix, from which we will create image we need to:

"we need to multiply x by the transpose of the rotation matrix, and then revert the centering and scaling steps. If we multiply by the transpose of the full rotation matrix, well recover the original data matrix exactly. But we can also choose to use just the first few principal components, instead."

By using X number of the principal components we are able to recieve back a dataset simmilar to original data. Of course tha bigger nr of components, the picture will be more precise.

I must admit, that in "reverese_PCA" part I used someones part of code*** as I struggled writing it and didnt came up with working solution.


#### Right now we can see how or picture look like with:10, 25, 50, 100 and 300 components.
```{r}
n_pcs <- c( 10, 25)
```

```{r,include=FALSE,echo=FALSE}
names(n_pcs) <- paste("First", n_pcs, "Components", sep = "_")

## map reverse_pca() 
recovered_MJs <- map_dfr(n_pcs, 
                          reverse_pca, 
                          .id = "pcs") %>%
  mutate(pcs = stringr::str_replace_all(pcs, "_", " "), 
         pcs = factor(pcs, levels = unique(pcs), ordered = TRUE))
```

```{r}
p <- ggplot(data = recovered_MJs, 
            mapping = aes(x = x, y = y, fill = value))
p_out <- p + geom_raster() + 
  scale_y_reverse() + 
  scale_fill_gradient(low = "black", high = "white") +
  facet_wrap(~ pcs, ncol = 2) + 
  guides(fill = FALSE) + 
  labs(title = "Recovering the content of an 800x600 pixel image\nfrom a Principal Components Analysis of its pixels") + 
  theme(strip.text = element_text(face = "bold", size = rel(1.2)),
        plot.title = element_text(size = rel(1.5)))
p_out
```

```{r}
n_pcs <- c( 50, 100)
```

```{r,include=FALSE,echo=FALSE}
names(n_pcs) <- paste("First", n_pcs, "Components", sep = "_")

## map reverse_pca() 
recovered_MJs <- map_dfr(n_pcs, 
                          reverse_pca, 
                          .id = "pcs") %>%
  mutate(pcs = stringr::str_replace_all(pcs, "_", " "), 
         pcs = factor(pcs, levels = unique(pcs), ordered = TRUE))
```

```{r}
p <- ggplot(data = recovered_MJs, 
            mapping = aes(x = x, y = y, fill = value))
p_out <- p + geom_raster() + 
  scale_y_reverse() + 
  scale_fill_gradient(low = "black", high = "white") +
  facet_wrap(~ pcs, ncol = 2) + 
  guides(fill = FALSE) + 
  labs(title = "Recovering the content of an 800x600 pixel image\nfrom a Principal Components Analysis of its pixels") + 
  theme(strip.text = element_text(face = "bold", size = rel(1.2)),
        plot.title = element_text(size = rel(1.5)))
p_out
```




```{r,include=FALSE,echo=FALSE}
n_pcs <- 300
names(n_pcs) <- paste("First", n_pcs, "Components", sep = "_")

## map reverse_pca() 
recovered_MJs <- map_dfr(n_pcs, 
                          reverse_pca, 
                          .id = "pcs") %>%
  mutate(pcs = stringr::str_replace_all(pcs, "_", " "), 
         pcs = factor(pcs, levels = unique(pcs), ordered = TRUE))

p <- ggplot(data = recovered_MJs, 
            mapping = aes(x = x, y = y, fill = value))
p_out <- p + geom_raster() + 
  scale_y_reverse() + 
  scale_fill_gradient(low = "black", high = "white") +
  facet_wrap(~ pcs, ncol = 2) + 
  guides(fill = FALSE) + 
  labs(title = "Recovering the content of an 800x600 pixel image\nfrom a Principal Components Analysis of its pixels") + 
  theme(strip.text = element_text(face = "bold", size = rel(1.2)),
        plot.title = element_text(size = rel(1.5)))
p_out

```

```{r}
n_pcs <- 300
p_out
```


As we can see, with first 20 components we are able to guess what is in the picture and with 30 components more we should be preety sure what the image presents. Of coures PCA can be performed also for colourful images.
As you can see there are a lot of applications of PCA technique. 






### References
- *https://builtin.com/data-science/step-step-explanation-principal-component-analysis
- ** https://builtin.com/data-science/step-step-explanation-principal-component-analysis
- *** https://www.r-bloggers.com/2019/10/reconstructing-images-using-pca/
- https://rpubs.com/Saskia/520216
- https://www.datavedas.com/dimensionality-reduction-in-r/
- https://stats.stackexchange.com/questions/229092/how-to-reverse-pca-and-reconstruct-original-variables-from-several-principal-com




